# TG-inferencing-with-context-work-in-progress-top11percenter
The LLM Science Exam contest entry which scored in the top 11% just short of bronze.
The primary reason for storing this notebook here is to serve as a reminder on how to build context for multiple choice questions to work with an LLM.
Specifically, the models supported are DeBERTa and LongFormer models.
With the number of Kaggle teams in this contest, medals were only available for the top 10%.

You'll need the data and models from the LLM Science Exam contest to run this notebook, plus one model from Hugging Face.
/kaggle/input/kaggle-llm-science-exam
/kaggle/input/2023kagglellm-deberta-v3-large-model1
/kaggle/input/all-paraphs-parsed-expanded
/kaggle/input/60k-data-with-context-v2
/kaggle/input/backup-806
/kaggle/input/blingfire-018
/kaggle/input/datasets-wheel
/kaggle/input/deberta-v3-large-freeze16-512
/kaggle/input/faiss-gpu-173-python310
/kaggle/input/llm-se-debertav3-large
/kaggle/input/llm-whls
/kaggle/input/deberta-llm-science-074
/kaggle/input/llm-science-run-context-2
/kaggle/input/longformer
/kaggle/input/longformer-race-model
/kaggle/input/longformer-large-race-60k-24-09-2023-3-26
/kaggle/input/sentencetransformers-allminilml6v2
/kaggle/input/sentence-transformers-222
/kaggle/input/stem-wiki-cohere-no-emb
/kaggle/input/wikipedia-2023-07-faiss-index
/kaggle/input/wikipedia-20230701
/kaggle/input/how-to-train-open-book-model-part-1
/kaggle/input/using-deepspeed-with-hf-trainer
/kaggle/input/llmse-ensembling-better-models

That's it.

